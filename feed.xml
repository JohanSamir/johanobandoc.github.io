<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mj10.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mj10.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-14T01:09:57+00:00</updated><id>https://mj10.github.io/feed.xml</id><title type="html">Moksh Jain</title><subtitle>Moksh Jain. PhD Student in Machine Learning at Mila and UdeM. </subtitle><entry><title type="html">GFlowNets and Scientific Discovery</title><link href="https://mj10.github.io/blog/2023/gflownets-scientific-discovery/" rel="alternate" type="text/html" title="GFlowNets and Scientific Discovery"/><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://mj10.github.io/blog/2023/gflownets-scientific-discovery</id><content type="html" xml:base="https://mj10.github.io/blog/2023/gflownets-scientific-discovery/"><![CDATA[<p>(This a high-level summary of our recent paper <d-cite key="jain2023gflownets"></d-cite>. This post was published on the <a href="https://m2d2.io/blog/posts/gflownets-and-scientific-discovery/">M2D2 Blog</a>.)</p> <h1 id="the-scientific-method">The Scientific Method</h1> <blockquote> <p>“<em>Science is often described as an iterative and cumulative process, a puzzle solved piece by piece, with each piece contributing a few hazy pixels of a much larger picture.”</em> — <em>Emperor of all Maladies,</em> Siddhartha Mukherjee</p> </blockquote> <p>The <strong>Scientific Method</strong> prescribes a systematic approach to gaining <strong>knowledge</strong> through observation, forming hypotheses, and experimentation. Popularized during the Renaissance, this principle has been at the core of the rapid technological growth that followed. Progress in science has led to technological advancement, which in turn has enabled further scientific progress, resulting in a continually improving “hazy picture” of the universe. Figure 1 shows a simplified version of the Scientific Method.</p> <p>To make the illustration concrete, consider the drug discovery process. It begins with the observation of a phenomenon in nature - the symptoms of a disease. These observations are then incorporated into our existing models of biology and medicine. Based on these observations and prior knowledge, several hypotheses can be formulated regarding the disease - the cause, mechanism of action, and potential therapies. These hypotheses are tested through experiments - detecting the presence of viral agents in affected organs, observing genetic pathways, testing therapies on isolated cells in-vitro, etc. At this point, completing the cycle, we return to the phase of observation, this time considering the effect of the designed experiment on the phenomenon. This cycle results in a constantly improving understanding of the phenomenon - improving our knowledge about biology and medicine, and increasingly precise and effective experiments - leading to better therapies.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/gfn_sd_fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/gfn_sd_fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/gfn_sd_fig1-1400.webp"/> <img src="/assets/img/blog/gfn_sd_fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">(Simplified) Illustration of the scientific method.</figcaption> </figure> <h3 id="experimentation-and-computation">Experimentation and Computation</h3> <p>The scientific method can be viewed as two complementary phases of <em>computation</em> and <em>experimentation.</em> Experimentation serves as an interface to the real world, where the phenomenon of interest is observed, intervened upon and its effects measured. Computation consists of analyzing the observations and experimental outcomes, formulating hypotheses and designing experiments to test said hypotheses. In reality the distinction between the two is often blurred. Computation and experiment have a symbiotic relationship - each one is incomplete in isolation without the other - which ultimately leads to progress. Historically, each of these phases can take considerable amounts of time. However, advancements in natural sciences have revolutionized the scale and precision with which experiments can be performed. At the same time advances in fields like machine learning have opened new avenues to accelerate computation. In this post, we focus on methods that enable us to accelerate the computation phase with data-driven approaches.</p> <h3 id="predictive-modelling-and-reasoning">Predictive Modelling and Reasoning</h3> <p>The computation phase deals with two distinct problems:</p> <ol> <li>Building models of the environment in which the phenomenon occurs: This approximate model should be expressive, capturing all aspects of the environment influencing the phenomenon. As the model will be built with finite experimental data, it should also be able to capture it’s <em>epistemic uncertainty.</em></li> <li>Reason about the phenomenon of interest and formulate hypotheses and design experiments: Leveraging the approximate model, we would like to come up with hypotheses and experiments about the phenomenon of interest.</li> </ol> <p>Recall the drug discovery example. Say we have identified a target protein responsible for the ailment, one can collect experimental data about the structure and binding behavior of the protein with ligands through in-vitro experiments, and build a computational model that captures this behavior, i.e. docking. Using this model, we can design ligands that can inhibit the activity of the said protein.</p> <p>In reality, however, each of the steps in the example above are extremely non-trivial, resulting in long timelines for drug discovery. Recent developments in machine learning are an exciting avenue, as they enable us to build large-scale complex models of physical systems, formulate hypotheses and design experiments to accelerate the computational phase of scientific discovery.</p> <h1 id="challenges">Challenges</h1> <p>In the last few decades, machine learning has enabled remarkable technological advances ranging from superhuman Go players to protein folding. These advances have been enabled, in part, by availability of extremely large datasets. A lot of the the approaches also assume the availability of a well specified objective to optimize. This leads us to two critical challenges in leveraging ML approaches for scientific discovery.</p> <h3 id="data">Data</h3> <p>The first critical challenge in leveraging learning based approaches for scientific discovery is that of <em>limited data.</em> By design, machine learning approaches rely on access to large datasets to extract <em>useful patterns.</em> But owing to fundamental limitations, it can be extremely expensive or impossible to obtain large amounts of data in many applications of interest. Going back to the drug discovery example, it can be extremely difficult to obtain experimental data for small-molecules binding with a target protein, at the scale required for machine learning methods. Limited data introduces uncertainty in the models we can learn, which needs to be accounted for when formulating hypotheses with the model, as it can useful for guiding the search for novel hypotheses and experiments to disambiguate them. Bayesian models offer a principled approach to deal with limited data by modelling the posterior over functions that fit the data, however, owing to approximations required to scale to realistic data, they can underestimate the true uncertainty. <d-cite key="cervera2021uncertainty"></d-cite></p> <h3 id="underspecification-and-diversity">Underspecification and Diversity</h3> <p>Machine learning approaches often assume access to some reward signal to evaluate quality of designs. For instance, for designing drug-like molecules, the true objective is to find drug-like molecules that inhibit the target protein <strong>within the human body.</strong> This objective, however, potentially cannot be specified as a simple scalar reward. In practice, the binding energy of the molecule with the target protein is used as the reward signal to search for molecules. The binding energy <em>alone</em> cannot not account for a lot of the factors that can influence the effect of the drug molecule within the human body. Thus, a molecule that just minimizes this binding energy can provide no effect in the actual environment. This makes it critical to find diverse hypotheses (in this case molecules) to account for the underspecification and uncertainty in the reward signal. Widely used approaches to tackle such problems like reinforcement learning and Bayesian optimization aim to discover a single maximizer of the the reward signal, not accounting for underspecification of the reward signal itself. <d-cite key="angermueller2019model,kim2022deep"></d-cite></p> <h1 id="gflownets">GFlowNets</h1> <p>Generative Flow Networks (GFlowNets) are a recently proposed probabilistic framework to tackle these challenges. Originally inspired by reinforcement learning, GFlowNets model the sequential generation of <em>compositional</em> objects through a sequence of actions. GFlowNets aim to generate these objects <em>proportional</em> to a some given reward signal.</p> <p>Consider a set of compositional objects \(\mathcal{X}\), for example, the set of all molecules \(50\) atoms. Each object \(x\in \mathcal{X}\) is composed of some building blocks \(\mathcal{A}\). In the molecule example, the building blocks consist of atoms and chemical bonds. Thus, each object \(x \in \mathcal{X}\) can be generated through a sequence of steps, where each step consists of adding a building block to an partially constructed object. In GFlowNets, we view this sequence of steps as a trajectory in \(\mathcal{G}\), a <em>weighted</em> <em>directed acyclic graph</em> (DAG), also known as a flow network in graph theory. The nodes of this graph, called states, consist of all possible objects that can be constructed using the blocks \(\mathcal{A}\), including an empty object \(s_0\) and partially-constructed. Any two states \(s, s'\) are connected by an edge \(s\rightarrow s'\) if there is a building block in \(\mathcal{A}\) that takes \(s\) to \(s'\). Note that building blocks available at each intermediate state can vary. In the molecule example, we cannot add a \(5^{th}\) bond to a carbon atom. Fully constructed objects \(\mathcal{X}\) are called terminal states i.e. have no outgoing edge, which in our molecule example corresponds to having the valency of all atoms satisfied. \(\mathcal{G}\) is acyclic since we are only allowed to add blocks, so we can never reach the same intermediate state again within a sequence.</p> <p>Starting at the empty state \(s_0\), we can generate an object \(x \in \mathcal{X}\), by traversing \(\mathcal{G}\) till we reach a terminal state. We call this a <em>complete trajectory</em>, \(\tau = (s_0\rightarrow s_1 \rightarrow \dots \rightarrow x)\). There can be several trajectories, all resulting in the same object \(x\). Given a reward function \(R: \mathcal{X} \mapsto \mathbb{R}^+\), GFlowNets learn a stochastic policy \(\pi\) to generate trajectories such that an object \(x\) is generated with a probability proportional to \(R(x), \pi(x) \propto R(x)\). This policy is defined using flows on \(\mathcal{G}\) which are learned based on a principle akin to conservation laws in physics. A brief primer on learning in GFlowNets is provided in an Appendix at the end of the post but I recommend <d-cite key="madan2023learning"></d-cite> for a detailed study on learning objectives in GFlowNets.</p> <p>This sampling of objects proportionally to the reward implicitly encourages generation of <strong>diverse</strong> and <strong>high reward</strong> objects, from different modes of the reward function. Within the context of the scientific discovery, GFlowNets can enable generation of <em>diverse, good</em> hypotheses and experiments, as well as building predictive models, discussed in the next section.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/gfn_sd_fig2.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/gfn_sd_fig2.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/gfn_sd_fig2.gif-1400.webp"/> <img src="/assets/img/blog/gfn_sd_fig2.gif" width="0.5cm" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Illustration of GFlowNets taken from <d-cite key="bengio2021blog"></d-cite>. The particles flowing through the graph represent the flow.</figcaption> </figure> <h3 id="why-gflownets">Why GFlowNets?</h3> <p>Let us look at how GFlowNets differ from other related conceptual frameworks:</p> <ul> <li><strong>Reinforcement Learning</strong>: GFlowNets learn policies to sample trajectories to match the reward of the terminal state rather than maximize it as in standard deep reinforcement learning</li> <li><strong>Markov Chain Monte-Carlo</strong>: GFlowNets amortize the computation during training so generating samples is fast, as opposed to MCMC methods where most of the computation happens during sampling. Additionally, GFlowNets exploit the generalization ability of neural networks potentially addressing the slow mode-mixing in MCMC methods.</li> <li><strong>Generative Models</strong>: Traditional generative models in deep learning such as VAEs require positive samples to model the distribution of interest, whereas GFlowNets use a reward function.</li> </ul> <p>GFlowNets roughly fall in the family of generalized variational inference methods and have strong connections to hierarchical variational models. <d-cite key="malkin2022gflownets,zimmermann2023a,zhang2022unifying"></d-cite> study the connections of GFlowNets to existing probabilistic modelling frameworks.</p> <p>To summarize, GFlowNets shine in problems with the following properties:</p> <ul> <li>There is compositional structure that can be exploited by sequential generation</li> <li>There is uncertainty associated with the reward, and thus diversity is important</li> <li>The reward function of interest is multi-modal.</li> </ul> <h2 id="learning-in-gflownets">Learning in GFlowNets</h2> <p>Let us look at how we can learn \(\pi\). Each complete trajectory in \(\mathcal{G}\) is assigned a <em>trajectory flow</em>, \(F(\tau)\). This flow represents the unnormalized probability mass associated with the trajectory. We can also define the <em>edge flow</em>, \(F(s \rightarrow s') = \sum_{s\rightarrow s' \in \tau}F(\tau)\), which is the sum of flows of all trajectories containing the edge. A key idea in GFlowNets is using the flows to drive the sequential generation of objects. To this end, using the flows, we can define a <em>forward policy</em> \(P_F(-|s)\), which describes how to choose the next next action (addition of a building block) at a state. This forward policy is defined as \(P_F(s'|s)= \frac{F(s\rightarrow s')}{\sum_{s''\in\text{Child}(s)}F(s\rightarrow s'')}\). </p> <p>We can generate trajectory \(\tau\) by iteratively sampling actions from the forward policy. As the actions at each state are assumed to be independent of the previous states, the likelihood of a trajectory under the forward policy is given by \(P_F(\tau) = \prod_{s\rightarrow s' \in \tau}P_F(s'|s)\). As noted earlier, there can be multiple trajectories resulting in the same object \(x\). The probability of generating an object \(x\) following \(P_F\), i.e. \(\pi(x)\) is given by \(\sum_{\tau=(s_0\rightarrow \dots\rightarrow x)}P_F(\tau)\), by considering all the trajectories resulting in \(x\). The learning problem in GFlowNets is to learn approximate flow functions such that the probability of generating \(x\), \(\pi(x)\) is proportional to its reward. </p> \[\pi(x) = \frac{R(x)}{Z}\] <p>When this equation is satisfied, \(Z\) denotes the partition function of the unnormalized distribution represented by the reward function, \(Z = \sum_{x\in\mathcal{X}}R(x)\). Approaches to tackle this learning problem generally involve learning an approximate flow function, and or approximate forward policies. These are approximated with neural networks operating on states \(s \in \mathcal{S}\).</p> <p><strong>Flow Matching</strong></p> <p>A flow \(F\) is <em>consistent</em> if the outgoing flow at each non-terminal state \(s\) matches the incoming flow.</p> \[\sum_{s''\in \text{Parent}(s)}F(s''\rightarrow s) = \sum_{s'\in \text{Child}(s)}F(s\rightarrow s')\] <p>This is similar to the notion of <em>feasible flows</em> in graph theory, and bears resemblance to the conservation laws in physics. Using this we can discuss a key result in GFlowNet, initially presented in <d-cite key="bengio2021flow"></d-cite></p> <details><summary>💡 <strong>Flow Matching Criterion</strong></summary> <p>Given a consistent flow \(F\), with the incoming flow at terminal states set equal to the reward, \(\sum_{s'\rightarrow x\in \mathcal{E}}F(s'\rightarrow x) = R(x)\), the marginal likelihood of sampling an object \(x\) is proportional to the reward \(\pi(x) = \frac{R(x)}{Z}\).</p> </details> <p>In other words if the flow is conserved at all states, then sampling trajectories following the flow results in reward proportional sampling. This elegant result leads to a relatively straightforward approach for tackling the GFlowNet learning problem - learn parameters \(\theta\) for the edge flow function \(F(s\rightarrow s';\theta)\), which is typically a neural network - resulting in the following flow matching objective</p> \[\mathcal{L}_{FM}(\tau;\theta) = \sum_{s\ne s_0\in\tau}\left(\sum_{s''\in \text{Parent}(s)}F(s''\rightarrow s;\theta) - R(s) - \sum_{s'\in \text{Child}(s)}F(s\rightarrow s';\theta)\right)^2\] <p>where \(R(s)\) is \(0\) for all terminal states and equal to the reward \(R(x)\) for the terminal states. We can already notice a key property of the learning objective - it is <em>off-policy.</em> What that means is that we can use any trajectory, not just ones sampled from the current policy, for training. This allows us to use exploratory policies to collect trajectories for training and even use offline data!</p> <p><strong>Subtrajectory Balance</strong></p> <p>Like temporal-difference learning objectives in RL, the flow matching objective, however, can suffer from slow credit assignment in long trajectories. This is addressed by the family of trajectory balance objectives. In particular, subtrajectory balance, introduced in <d-cite key="madan2023learning,malkin2022trajectory"></d-cite>, is a learning objective which captures several other GFlowNet learning objectives. Before we look at the subtrajectory balance objective, we define the <em>backward policy \(P_B\)</em> which is a necessary ingredient. Like the forward policy \(P_F\) defines a distribution over the children of a state, \(P_B\) defines a distribution over the parents of a state. With \(P_B\) we can generate a trajectory backwards starting at a terminal states \(x\). Let us now look at the subtrajectory balance objective</p> \[\mathcal{L}_{SubTB}(\tau=(s_m\rightarrow \dots\rightarrow s_n);\theta) = \left(\log\frac{F(s_m;\theta\prod_{i=m}^{n-1}P_F(s_{i+1}|s_{i};\theta)}{F(s_n;\theta\prod_{i=m}^{n-1}P_B(s_{i}|s_{i+1};\theta)} \right)^2\] <p>where \(\theta\) are the learnable parameters for \(P_F, P_B, F\). An interesting property of the objective is that it can operates on any subtrajectory. During training we consider subtrajectories from trajectroies sampled using the current policy. As opposed to the flow matching loss where credit is propagated over multiple trajectories, here the credit is assigned to all states in the subtrajectory resulting in lower variance in the gradients and faster convergence. I refer the curious readers to <d-cite key="madan2023learning">&lt;/d=cite&gt; for a detailed look at learning objectives for GFlowNets.</d-cite></p> <p>We can define a general training algorithm for GFlowNets as follows</p> <ul> <li>Initialize parameters \(\theta\) for the flow function \(F(;\theta)\) and/or policies \(P_F(-|-; \theta), P_B(-|-;\theta), Z_\theta\)</li> <li>Sample trajectories following the forward policy \(\tau\sim P_F\)</li> <li>Compute loss for the trajectory \(\mathcal{L}(\tau;\theta)\)</li> <li>Update parameters \(\theta\) with gradients from the loss.</li> </ul> <p>There is a growing literature around the mathematical foundations of GFlowNets which is too extensive to cover in a single post. I recommend <d-cite key="bengio2021gflownet"></d-cite> for a deeper mathematical study of GFlowNets.</p> <h1 id="promise-of-gflownets-for-scientific-discovery">Promise of GFlowNets for Scientific Discovery</h1> <h3 id="generating-diverse-and-useful-experiments">Generating Diverse and Useful Experiments</h3> <p>Our initial work on GFlowNets <d-cite key="bengio2021flow"></d-cite> was motivated by the problem of diverse molecule generation. In particular, the goal was to generate molecules that bind to a particular target, and potentially inhibit the activity of the target. We considered soluble epoxide hydrolase (sEH) in it’s 4JNC configuration, which plays a role in certain respiratory and heart diseases, as the target. To simulate the action of the designed ligand on the sEH target, we relied on docking simulations from Autodock Vina. However, each simulation takes about 5 minutes to run, thus learning a policy with the docking score as reward directly would be prohibitively expensive. Instead, we train a graph neural network, which is much faster to query for a reward, using a dataset of docking scores for 300,000 molecules which is used a <em>proxy</em> for the true reward.</p> <p>We looked at fragment based generation of molecules - the policy picks molecular subgraphs, rather than individual atoms, to put together for constructing the molecule graph. These fragments are generated from the Zinc database. This problem possesses all the 3 properties for GFlowNets to be effective - compositionality - the molecules are composed of the subgraphs with unique chemical properties, uncertainty in the reward - the reward is approximated by a neural network which will have some uncertainty associated to it, and the reward is multimodal - we can expect several molecules to bind well to a given target. Consequently, GFlowNets are able to substantially outperform other methods, generating <strong><em>diverse</em></strong> molecules with <strong>low binding energy</strong>. In particular GFlowNets discover significantly more modes of the reward function relative to other methods. Further we also consider an active learning setup where we start with a dataset of 2000 molecules and use the docking simulation as the oracle. We find that generating batches to be queried with GFlowNets results in significant performance improvements, resulting in discovery of much lower energy molecules than those in the initial dataset.</p> <p>Further exploring the active learning setting, we considered additional improvements to GFlowNets to improve performance in the active learning setting <d-cite key="jain2022biological"></d-cite>. The improvements were two-fold: (a) incorporating offline data to improve sample efficiency, and (b) incorporating the epistemic uncertainty from the approximate reward model to guide search to novel areas of the state space. With these improvements we demonstrated that GFlowNets can generate novel and diverse biological sequences, in particular antimicrobial peptides, which have significant potential for therapeutic use.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/gfn_sd_fig3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/gfn_sd_fig3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/gfn_sd_fig3-1400.webp"/> <img src="/assets/img/blog/gfn_sd_fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Schematic of GFlowNet-AL, incorporating GFlowNets of generation of diverse candidates in active learning.</figcaption> </figure> <p>In many practical settings, there can be multiple properties of interest, where we want to generate experiments which capture them simultaneously. For example, in the context of drug discovery we would like to generate molecules that inhibit the target but which are also synthesizable and not toxic to humans. Multi-Objective GFlowNets <d-cite key="jain2023multi"></d-cite> are extensions of GFlowNets to tackle problems with multiple objectives being optimized simultaneously. Multi-Objective GFlowNets decompose the multi-objective optimization problem into a family of sub-problems which can modelled simultaneously. Through experiments on a wide variety of tasks ranging from small molecule generation to protein design, we show that Multi-Objective GFlowNets generate <em>diverse and Pareto-optimal</em> candidates in multi-objective optimization. In particular, we show that GFlowNets can generate molecules that bind to a target while being synthesizable and having drug-like properties. These initial empirical results have demonstrated the ability of GFlowNets to have significant impact in realistic experimental design scenarios.</p> <h3 id="learning-predictive-models">Learning Predictive Models</h3> <p>While still in early stages, recent work has also demonstrated the ability of GFlowNets to model predictive posteriors from data.</p> <p>DAG-GFlowNets<d-cite key="deleu2022bayesian"></d-cite> demonstrated how GFlowNets can be used to model Bayesian posteriors. They study the problem of learning the Bayesian posterior over graphical models that fit well the data. Through experiments on standard causal discovery tasks, they establish the ability of GFlowNets to accurately model the posterior over the structure of the underlying causal graph of the data. This posterior over the causal structure can in-turn enable targeted uncertainty estimation. For example, given gene knockout data, the posterior over causal graphs that fit the data can reveal the uncertainty in our model about specific causal links.</p> <p>Another popular paradigm for practical scenarios is that of approximate Bayesian inference. Methods such as Monte-Carlo dropout and ensembles have become default choices to represent the uncertainty over neural network parameters. Pushing this direction further, our recent work <d-cite key="liu2023gflowout"></d-cite> leverages GFlowNets to generate dropout masks, to obtain a more faithful and reliable predictive posterior, by generating <strong>diverse</strong> dropout masks.</p> <h2 id="looking-forward">Looking Forward</h2> <p>With this glowing discussion and <em>diverse</em> range of applications, you might be tempted to ask “Are GFlowNets all you need?” But as with most questions of this form, the answer is certainly “No”.</p> <p>What GFlowNets <em>do</em> offer is a flexible probabilistic modeling framework that paves the way for developing approaches to accelerate scientific discovery. There are still several open challenges that need to be addressed in the context of GFlowNets: Multi-fidelity GFlowNets, Intervention Policies, Exploration, and many more! We expand on these ideas, positioning GFlowNets as a key tool to accelerate scientific discovery in <d-cite key="jain2023gflownets"></d-cite>.</p> <p>To get started with GFlowNets, checkout <a href="https://colab.research.google.com/drive/1fUMwgu2OhYpQagpzU5mhe9_Esib3Q2VR">this tutorial Colab</a> by Emmanuel Bengio, and this list of <a href="https://github.com/zdhNarsil/Awesome-GFlowNets">Awesome-GFlowNets</a> by Dinghuai Zhang!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Yoshua Bengio and Emmanuel Bengio provided valuable comments and feedback on this post. My collaborators and mentors helped shape the ideas discussed here through numerous discussions.</p>]]></content><author><name>Moksh Jain</name></author><summary type="html"><![CDATA[A high level overview of GFlowNets and their potential to accelerate scientific discovery.]]></summary></entry><entry><title type="html">Learning Disentangled Representations</title><link href="https://mj10.github.io/blog/2019/learning-disentangled-representations/" rel="alternate" type="text/html" title="Learning Disentangled Representations"/><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><id>https://mj10.github.io/blog/2019/learning-disentangled-representations</id><content type="html" xml:base="https://mj10.github.io/blog/2019/learning-disentangled-representations/"><![CDATA[<p><em>You can find the interactive notebook accompanying this article</em> <a href="https://colab.research.google.com/drive/1RPzxB9DZnQmoggIrwTk_c_8DdZqRXC2p"><strong>here</strong></a>.</p> <p>A <strong>representation</strong> in the most vague sense refers to the lower dimensional projection of some high-dimensional input. A good representation can then be defined as one that captures the relevant information required to describe the original high-dimensional data in a much more compact way (i.e \(num\_features\) « \(input\_dims\) ). There has been a lot of interest in the Machine Learning community to build models that can learn useful representations from high dimensional sensory inputs like audio, video, text, images, etc. These representations can then be used to have further models to perform useful tasks, like classifying images. The basic idea is having lower dimensional representations that can describe the original data is useful for models to extract more useful information than the original higher dimensional data. Representation Learning has become an important research area in the recent years. In their survey, <a href="https://arxiv.org/abs/1206.5538">Bengio et al.</a> talk about the need for representation learning and the latest developments in the area. According to the survey, informally, the goal of representation learning is to find useful transformations \(r(x)\) of the higher dimensional data \(x\) which makes it easier to extract useful information for various predictors. However, since the survey was published a lot of work has been done in this area, and one of the focuses has been of learning disentangled representations.</p> <h2 id="what-is-a-disentangled-representation">What is a disentangled representation?</h2> <p>One of the underlying asumptions in representation learning is that the high dimensional sensory data in the real world \(x\), like an image, is generated by a 2-step generative process. The first step is sampling a semantically meaningful latent variable \(z\) (from \(P(z)\)) that describes the high level information of the data, for example the location of a flower in the image, the color of the flower, it’s shape etc. The final step is to sample the actual observation \(x\) from the conditional distribution \(P(x|z)\). This essentially means that the high dimensional observation \(x\) can be explained semantically by the lower dimensional representation \(z\). <a href="https://arxiv.org/abs/1811.12359">Locatello et al</a>., suggest a few characteristics for a \(disentangled\) \(representation\) \(z\):</p> <ul> <li>contain all information in \(x\) in a compact and interpretable structure</li> <li>independent of the task being performed (eg. classification, etc)</li> <li>should be useful for (semi-)supervised learning of downstream tasks, transfer and few shot learning</li> <li>They should enable to integrate out nuisance factors, to perform interventions, and to answer counterfactual questions.</li> </ul> <p>The intuitive explanation adopted for disentangled representations is as follows: <em>a disentangled representation should separate the distinct, informative factors of variations in the data</em>. That is, changing one factor (\(z_i\)) in \(z\) should result in only a single factor in \(x\). In essence, if one feature in the representation changes it only affects one semantic feature of the observation. Let us consider the example of an image with an object. A <em>good</em> disentangled representation in this case would capture the location (xy-coordinates), shape, color and size as the <em>factors of variation</em>. This is a good disentangled representation since, changing on of the factors (let’s say the color) affects only the color and not the shape, size or location.</p> <p>This however is just a loose conceptual intuition behind the idea of disentangled representation. In fact, until recently there was no widely agreed upon solid definition for disentangled representations. Instead there were a number of different metrics proposed over the years that would capture these intuitions. Recently, <a href="https://arxiv.org/abs/1812.02230">Higgins, Amos et al.</a> proposed a formal definition of disentangled representations using the idea of symmetry transforms and from group and representation theory. This formalism helps in setting up a concrete definition for the problem being solved and helps in evaluating and understanding approaches to solve the problem. Their definition is as follows:</p> <p><strong><em>A vector representation is called a disentangled representation with respect to a particular decomposition of a symmetry group into subgroups, if it decomposes into independent subspaces, where each subspace is affected by the action of a single subgroup, and the actions of all other subgroups leave the subspace unaffected.</em></strong></p> <p>A symmetry transform of an object is a \(transformation\) that leaves certain properties of the object \(invariant\). For example, translation and rotation are symmetries of objects – an apple is still an apple whether it is placed on a table or in a bag, and whether it rolls on its side or remains upright. The set of such transformations forms the \(symmetry\) \(group\) and the effects of these transformations are the \(actions\) of the symmetry group on the world state(Note: this the underlying world state and not the observation \(x\)). The actions that change only a certain aspect of the world state while keeping others fixed is a \(disentangled\) \(group\) \(action\). So for example changing the horizontal position of apple only affects it’s horizontal position and not it’s vertical position or color, etc. Another thing we notice from this is that we can decompose this symmetry group into \(symmetry\) \(subgroups\). So in the example of the apple, horizontal transformation could be one such subgroup. Here the horizontal subspace is affected only by actions of the horizontal translation subgroup. So far we talked about the underlying abstract world state. To generalise to observations, we assume there is a generative process that generates the dataset of observations from a given set of underlying world states. In some situations, it is possible to find a composite mapping between the disentangled group actions in the abstract state space to the transformations in the vector space of the representation. In short, we can call a representation \(disentangled\) if the vector space of the representation can be decomposed into independent subspaces such that each subspace is only affected by a single symmetry subgroup, which in turn is a set of symmetry transformations that affect only a certain aspect of the world state. The paper decribes the formalism in further detail and also discusses link between the proposed definition and the currently generally accepted intuitive ideas about disentangled representations.</p> <p>One might question how are these representations useful? As we saw previously, disentangled representations capture independent features that describe a single aspect of the observation. This characteristic is useful in enabling generalisation to previously unobserved situations, since a model can extract meaningful information about the observation to understand it from the disentangled representation. Approches using disentangled representations have found a lot of successs in various tasks including <a href="https://arxiv.org/abs/1807.01521">curiousity driven exploration</a>, <a href="https://arxiv.org/abs/1811.04784">abstract reasoning</a>, <a href="https://arxiv.org/pdf/1707.03389.pdf">visual concept learning</a> and <a href="https://arxiv.org/pdf/1707.08475.pdf">domain adaptation in reinforcement learning</a>.</p> <h2 id="how-to-learn-these-disentangled-representations">How to learn these disentangled representations?</h2> <p>Learning disentangled representations is at it’s core a type of dimensionality reduction problem. The distinction here from other forms of dimensionality reduction is that there are certain restrictions on the vector space of the learned representation. Unsupervised learning of these representations is an interesting problem since it would allow models to learn from huge troves of available unlabelled data. Thus, there has been a lot of interest in the machine learning community to design unsupervised learning algorithms to learn these representations. Variants of variational autoencoders (proposed by <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling</a> in 2013) have seen quite a lot of success in recent years in tackling this problem, and provide state of the art performance in unsupervised learning of disentangled representations. Variational Autoencoders can be seen as modelling the 2-step generative process described above. A specific prior \(P(z)\) is selected, and then the distribution \(P(x|z)\) is parameterized using a deep neural network. The goal is to infer good values of the latent variables given observed data, which is essentially computing the posterior \(P(z|x)\). This distribution \(P(z|x)\) is approximated using a variational distribution \(Q(z|x)\) which is also parametrized by a neural network. The representation is usually taken to be the mean of \(Q(z|x)\). We discuss the specifics of VAEs in later sections. Several models based on this, such as BetaVAE, FactorVAE, and AnnealedVAE among others, have been introduced to learn disentangled representations, and provide state-of-the-art performance.</p> <p>However, in their recent work, <a href="https://arxiv.org/pdf/1811.12359.pdf">Locatello et al.</a> perform a large systematic study of these models to evaluate the recent progress in the area. Their study had a few key findings:</p> <ul> <li>They found no empirical evidence that the considered models can be used to reliably learn disentangled representations in an unsupervised way, since random seeds and hyperparameters seem to matter more than the model choice. That is, even if a large number of models are trained with some of them being disentangled, these disentangled representations cannot be identified without access to ground-truth labels.</li> <li>Good hyperparameter values do not appear to consistently transfer across the datasets.</li> <li>They were not able to validate the assumption that disentanglement is useful for downstream tasks, e.g., few-shot learning with disentangled representations.</li> </ul> <p>In addition to these findings, they also present the <em>Impossibilty Result</em> which states the following: <em><strong>unsupervised learning of disentangled representations is impossible without inductive biases on both the data set and the models</strong></em>. So it is impossible to learn disentangled representations without making certain assumptions on the dataset and incorporating them in the model, which essentially restricts generalizability of models across datasets. They also propose observations for future research on the topic and to that end released the <a href="https://github.com/google-research/disentanglement_lib/"><code class="language-plaintext highlighter-rouge">disentanglement_lib</code></a> with all the models used in their study to aid in future research in topic, along with the <a href="https://www.aicrowd.com/challenges/neurips-2019-disentanglement-challenge">NeurIPS 2019: Disentanglement Challenge</a> to accelerate research in the area.</p> <h2 id="variational-autoencoders">Variational Autoencoders</h2> <p>As discussed in the previous sections, we start by assuming a specific prior \(p(z)\) on the latent space, parametrizing the distribution \(p(x|z)\) using a neural network, and approximating the posterior \(p(z|x)\) with a neural network parameterized variational distribution \(q(z|x)\). Now we discuss the motivations behind this model and how we train these models.</p> <p><span>What we want the model to do is to learn how to generate the representation given the data as input, i.e compute \(p(z|x)\), and also the model should be able to generate the data given the latent representation (compute \(p(x|z)\)). We start by sampling \(z\) from the prior \(p(z)\). The likelihood of the data conditioned to latent variable \(z\) is \(p(x|z)\). The joint distribution \(p(x, z)\) can be decomposed as \(p(x,z) = p(x|z)p(z)\). Now at first glance calculating the posterior \(p(z|x)\) might seem straightforward using the Bayes rule: \(p(z|x) = \frac{p(x|z)p(z)}{p(x)}\)</span></p> <p><span>However, computing \(p(x)=\int p(x|z)p(z)dz\) is not computationally tractable. Thus, we approximate the posterior \(p(z|x)\) with a family of distributions \(q_\lambda (z|x)\) (here \(\lambda\) is used as an index for the distributions). Kullback-Leibler divergence(KL divergence) is used to measure how different a probability distribution is from another given probability distribution. We use this to evaluate how well \(q_\lambda (z|x)\) approximates \(p(z|x)\). Our goal would be to have the distributions be as similar as possible, so we minimize the KL-divergence.</span></p> \[\mathbb{KL}(q_\lambda (z|x)\ ||\ p(z|x)) = \mathbf{E}_q[\log q_\lambda (z|x)] - \mathbf{E}_q[\log p(x, z)] + \log p(x)\] <p>But we encounter \(p(x)\) once again. To get around this we use the ELBO (Evidence Lower Bound).</p> \[ELBO(\lambda) = \mathbf{E}_q[\log p(x, z)] - \mathbf{E}_q[\log q_\lambda (z|x)]\] <p>Thus from these two equations we get the following:</p> \[\log p(x) = ELBO(\lambda) + \mathbb{KL}(q_\lambda (z|x)\ ||\ p(z|x))\] <p>Since the Jensen inequality states that the KL divergence is always \(\geq 0\), KL-divergence can be minimized by maximizing ELBO (as \(p(x)\) doesn’t change). Maximizing the ELBO is computationally tractable, thus we can train the model with the objective of maximizing ELBO. Now, since no datapoint shares its latent \(z\) with the latent variable of another datapoint, we can decompose ELBO into a sum such that each term depends on one datapoint.</p> \[ELBO_i(\lambda)=\mathbf{E}_{q_\lambda} [\log p(x_i | z)] - \mathbb{KL}(q_\lambda (z|x_i) || p(z))\] <p>This value can be interpreted as follows: The first term is the reconstruction loss for the datapoints (i.e. get \(z\) from \(x\) and then obtain \(x'\) and compare \(x\) and \(x'\)) and the KL-divergence term acts as a sort of regularizer.</p> <p><span>As mentioned previously, the distrbutions can be parametrized by neural networks. So we start with the approximate posterior, which is also called encoder as it encodes the input data into the latent variable \(q_\theta (z|x, \lambda)\)(where \(\theta\) indicates the neural network weights), which outputs the \(\lambda\) for a given datapoint \(x\). As mentioned earlier \(\lambda\) is an index over the family of distrbutions \(q\), so we use \(\lambda\) to get the required distribution and sample the latent representation \(z\) from it. For example if we select a family of gaussians then \(\lambda\) would be the mean and variance of the distributions. Once we have \(z\) we obtain the reconstruction from the ‘decoder’, \(p_\phi (x|z)\). And the loss function is \(-ELBO\) which we can minimize using stochastic gradient descent. </span></p> <p><span>This was the general idea behind a variational autoencoder. Now to allow these models to learn disentangled representations, the general approach is to enforce a factorized aggregated posterior \(\int q(z|x)p(x)dx\) to encourage disentanglement. All of the approaches try to enforce this in some way by either modifying the regularizer or having additional objectives or by some architectural choices. </span></p> <h2 id="summary">Summary</h2> <p>In this post we discussed what are disentangled representations, what are autoencoders, and how we can use variational autoencoders to learn disentangled representations. In the accompanying notebook we demonstrate how to get started by building a custom VAE with the disentanglement_lib, evaluating it and visualising it. If you are interested in disentangled representations, do consider participating in the <a href="https://www.aicrowd.com/challenges/neurips-2019-disentanglement-challenge">NeurIPS 2019: Disentanglement Challenge</a>.</p> <h2 id="references">References</h2> <p>Locatello, Francesco et al. <a href="https://arxiv.org/abs/1811.12359">“Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.”</a> ICML (2018).</p> <p>Higgins, Irina et al. <a href="https://arxiv.org/abs/1812.02230">“Towards a Definition of Disentangled Representations.”</a> ArXiv abs/1812.02230 (2018)</p> <p><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Tutorial - What is a variational autoencoder? - Jaan Alatosaar</a></p> <p><a href="https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html">Google AI Blog: Evaluating the Unsupervised Learning of Disentangled Representations</a></p>]]></content><author><name>Moksh Jain</name></author><summary type="html"><![CDATA[You can find the interactive notebook accompanying this article here.]]></summary></entry></feed>