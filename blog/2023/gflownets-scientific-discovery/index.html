<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>GFlowNets and Scientific Discovery | Moksh Jain</title> <meta name="author" content="Moksh Jain"> <meta name="description" content="A high level overview of GFlowNets and their potential to accelerate scientific discovery."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="../../../assets/css/main.css"> <link rel="canonical" href="index.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="../../../assets/js/theme.js"></script> <script src="../../../assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="../../../assets/js/distillpub/template.v2.js"></script> <script src="../../../assets/js/distillpub/transforms.v2.js"></script> <script src="../../../assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "GFlowNets and Scientific Discovery",
      "description": "A high level overview of GFlowNets and their potential to accelerate scientific discovery.",
      "published": "March 7, 2023",
      "authors": [
        {
          "author": "Moksh Jain",
          "authorURL": "https://mj10.github.io/",
          "affiliations": [
            {
              "name": "Université de Montréal, Mila - Quebec AI Institute",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="../../../index.html">Moksh Jain</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="../../../index.html">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="../../index.html">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="../../../publications/index.html">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="../../../cv/index.html">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>GFlowNets and Scientific Discovery</h1> <p>A high level overview of GFlowNets and their potential to accelerate scientific discovery.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="index.html#the-scientific-method">The Scientific Method</a></div> <div><a href="index.html#challenges">Challenges</a></div> <div><a href="index.html#gflownets">GFlowNets</a></div> <div><a href="index.html#promise-of-gflownets-for-scientific-discovery">Promise of GFlowNets for Scientific Discovery</a></div> </nav> </d-contents> <p>(This a high-level summary of our recent paper <d-cite key="jain2023gflownets"></d-cite>. This post was published on the <a href="https://m2d2.io/blog/posts/gflownets-and-scientific-discovery/" rel="external nofollow noopener" target="_blank">M2D2 Blog</a>.)</p> <h1 id="the-scientific-method">The Scientific Method</h1> <blockquote> <p>“<em>Science is often described as an iterative and cumulative process, a puzzle solved piece by piece, with each piece contributing a few hazy pixels of a much larger picture.”</em> — <em>Emperor of all Maladies,</em> Siddhartha Mukherjee</p> </blockquote> <p>The <strong>Scientific Method</strong> prescribes a systematic approach to gaining <strong>knowledge</strong> through observation, forming hypotheses, and experimentation. Popularized during the Renaissance, this principle has been at the core of the rapid technological growth that followed. Progress in science has led to technological advancement, which in turn has enabled further scientific progress, resulting in a continually improving “hazy picture” of the universe. Figure 1 shows a simplified version of the Scientific Method.</p> <p>To make the illustration concrete, consider the drug discovery process. It begins with the observation of a phenomenon in nature - the symptoms of a disease. These observations are then incorporated into our existing models of biology and medicine. Based on these observations and prior knowledge, several hypotheses can be formulated regarding the disease - the cause, mechanism of action, and potential therapies. These hypotheses are tested through experiments - detecting the presence of viral agents in affected organs, observing genetic pathways, testing therapies on isolated cells in-vitro, etc. At this point, completing the cycle, we return to the phase of observation, this time considering the effect of the designed experiment on the phenomenon. This cycle results in a constantly improving understanding of the phenomenon - improving our knowledge about biology and medicine, and increasingly precise and effective experiments - leading to better therapies.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/gfn_sd_fig1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/gfn_sd_fig1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/gfn_sd_fig1-1400.webp"></source> <img src="../../../assets/img/blog/gfn_sd_fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">(Simplified) Illustration of the scientific method.</figcaption> </figure> <h3 id="experimentation-and-computation">Experimentation and Computation</h3> <p>The scientific method can be viewed as two complementary phases of <em>computation</em> and <em>experimentation.</em> Experimentation serves as an interface to the real world, where the phenomenon of interest is observed, intervened upon and its effects measured. Computation consists of analyzing the observations and experimental outcomes, formulating hypotheses and designing experiments to test said hypotheses. In reality the distinction between the two is often blurred. Computation and experiment have a symbiotic relationship - each one is incomplete in isolation without the other - which ultimately leads to progress. Historically, each of these phases can take considerable amounts of time. However, advancements in natural sciences have revolutionized the scale and precision with which experiments can be performed. At the same time advances in fields like machine learning have opened new avenues to accelerate computation. In this post, we focus on methods that enable us to accelerate the computation phase with data-driven approaches.</p> <h3 id="predictive-modelling-and-reasoning">Predictive Modelling and Reasoning</h3> <p>The computation phase deals with two distinct problems:</p> <ol> <li>Building models of the environment in which the phenomenon occurs: This approximate model should be expressive, capturing all aspects of the environment influencing the phenomenon. As the model will be built with finite experimental data, it should also be able to capture it’s <em>epistemic uncertainty.</em> </li> <li>Reason about the phenomenon of interest and formulate hypotheses and design experiments: Leveraging the approximate model, we would like to come up with hypotheses and experiments about the phenomenon of interest.</li> </ol> <p>Recall the drug discovery example. Say we have identified a target protein responsible for the ailment, one can collect experimental data about the structure and binding behavior of the protein with ligands through in-vitro experiments, and build a computational model that captures this behavior, i.e. docking. Using this model, we can design ligands that can inhibit the activity of the said protein.</p> <p>In reality, however, each of the steps in the example above are extremely non-trivial, resulting in long timelines for drug discovery. Recent developments in machine learning are an exciting avenue, as they enable us to build large-scale complex models of physical systems, formulate hypotheses and design experiments to accelerate the computational phase of scientific discovery.</p> <h1 id="challenges">Challenges</h1> <p>In the last few decades, machine learning has enabled remarkable technological advances ranging from superhuman Go players to protein folding. These advances have been enabled, in part, by availability of extremely large datasets. A lot of the the approaches also assume the availability of a well specified objective to optimize. This leads us to two critical challenges in leveraging ML approaches for scientific discovery.</p> <h3 id="data">Data</h3> <p>The first critical challenge in leveraging learning based approaches for scientific discovery is that of <em>limited data.</em> By design, machine learning approaches rely on access to large datasets to extract <em>useful patterns.</em> But owing to fundamental limitations, it can be extremely expensive or impossible to obtain large amounts of data in many applications of interest. Going back to the drug discovery example, it can be extremely difficult to obtain experimental data for small-molecules binding with a target protein, at the scale required for machine learning methods. Limited data introduces uncertainty in the models we can learn, which needs to be accounted for when formulating hypotheses with the model, as it can useful for guiding the search for novel hypotheses and experiments to disambiguate them. Bayesian models offer a principled approach to deal with limited data by modelling the posterior over functions that fit the data, however, owing to approximations required to scale to realistic data, they can underestimate the true uncertainty. <d-cite key="cervera2021uncertainty"></d-cite></p> <h3 id="underspecification-and-diversity">Underspecification and Diversity</h3> <p>Machine learning approaches often assume access to some reward signal to evaluate quality of designs. For instance, for designing drug-like molecules, the true objective is to find drug-like molecules that inhibit the target protein <strong>within the human body.</strong> This objective, however, potentially cannot be specified as a simple scalar reward. In practice, the binding energy of the molecule with the target protein is used as the reward signal to search for molecules. The binding energy <em>alone</em> cannot not account for a lot of the factors that can influence the effect of the drug molecule within the human body. Thus, a molecule that just minimizes this binding energy can provide no effect in the actual environment. This makes it critical to find diverse hypotheses (in this case molecules) to account for the underspecification and uncertainty in the reward signal. Widely used approaches to tackle such problems like reinforcement learning and Bayesian optimization aim to discover a single maximizer of the the reward signal, not accounting for underspecification of the reward signal itself. <d-cite key="angermueller2019model,kim2022deep"></d-cite></p> <h1 id="gflownets">GFlowNets</h1> <p>Generative Flow Networks (GFlowNets) are a recently proposed probabilistic framework to tackle these challenges. Originally inspired by reinforcement learning, GFlowNets model the sequential generation of <em>compositional</em> objects through a sequence of actions. GFlowNets aim to generate these objects <em>proportional</em> to a some given reward signal.</p> <p>Consider a set of compositional objects \(\mathcal{X}\), for example, the set of all molecules \(50\) atoms. Each object \(x\in \mathcal{X}\) is composed of some building blocks \(\mathcal{A}\). In the molecule example, the building blocks consist of atoms and chemical bonds. Thus, each object \(x \in \mathcal{X}\) can be generated through a sequence of steps, where each step consists of adding a building block to an partially constructed object. In GFlowNets, we view this sequence of steps as a trajectory in \(\mathcal{G}\), a <em>weighted</em> <em>directed acyclic graph</em> (DAG), also known as a flow network in graph theory. The nodes of this graph, called states, consist of all possible objects that can be constructed using the blocks \(\mathcal{A}\), including an empty object \(s_0\) and partially-constructed. Any two states \(s, s'\) are connected by an edge \(s\rightarrow s'\) if there is a building block in \(\mathcal{A}\) that takes \(s\) to \(s'\). Note that building blocks available at each intermediate state can vary. In the molecule example, we cannot add a \(5^{th}\) bond to a carbon atom. Fully constructed objects \(\mathcal{X}\) are called terminal states i.e. have no outgoing edge, which in our molecule example corresponds to having the valency of all atoms satisfied. \(\mathcal{G}\) is acyclic since we are only allowed to add blocks, so we can never reach the same intermediate state again within a sequence.</p> <p>Starting at the empty state \(s_0\), we can generate an object \(x \in \mathcal{X}\), by traversing \(\mathcal{G}\) till we reach a terminal state. We call this a <em>complete trajectory</em>, \(\tau = (s_0\rightarrow s_1 \rightarrow \dots \rightarrow x)\). There can be several trajectories, all resulting in the same object \(x\). Given a reward function \(R: \mathcal{X} \mapsto \mathbb{R}^+\), GFlowNets learn a stochastic policy \(\pi\) to generate trajectories such that an object \(x\) is generated with a probability proportional to \(R(x), \pi(x) \propto R(x)\). This policy is defined using flows on \(\mathcal{G}\) which are learned based on a principle akin to conservation laws in physics. A brief primer on learning in GFlowNets is provided in an Appendix at the end of the post but I recommend <d-cite key="madan2023learning"></d-cite> for a detailed study on learning objectives in GFlowNets.</p> <p>This sampling of objects proportionally to the reward implicitly encourages generation of <strong>diverse</strong> and <strong>high reward</strong> objects, from different modes of the reward function. Within the context of the scientific discovery, GFlowNets can enable generation of <em>diverse, good</em> hypotheses and experiments, as well as building predictive models, discussed in the next section.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/gfn_sd_fig2.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/gfn_sd_fig2.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/gfn_sd_fig2.gif-1400.webp"></source> <img src="../../../assets/img/blog/gfn_sd_fig2.gif" width="0.5cm" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Illustration of GFlowNets taken from <d-cite key="bengio2021blog"></d-cite>. The particles flowing through the graph represent the flow.</figcaption> </figure> <h3 id="why-gflownets">Why GFlowNets?</h3> <p>Let us look at how GFlowNets differ from other related conceptual frameworks:</p> <ul> <li> <strong>Reinforcement Learning</strong>: GFlowNets learn policies to sample trajectories to match the reward of the terminal state rather than maximize it as in standard deep reinforcement learning</li> <li> <strong>Markov Chain Monte-Carlo</strong>: GFlowNets amortize the computation during training so generating samples is fast, as opposed to MCMC methods where most of the computation happens during sampling. Additionally, GFlowNets exploit the generalization ability of neural networks potentially addressing the slow mode-mixing in MCMC methods.</li> <li> <strong>Generative Models</strong>: Traditional generative models in deep learning such as VAEs require positive samples to model the distribution of interest, whereas GFlowNets use a reward function.</li> </ul> <p>GFlowNets roughly fall in the family of generalized variational inference methods and have strong connections to hierarchical variational models. <d-cite key="malkin2022gflownets,zimmermann2023a,zhang2022unifying"></d-cite> study the connections of GFlowNets to existing probabilistic modelling frameworks.</p> <p>To summarize, GFlowNets shine in problems with the following properties:</p> <ul> <li>There is compositional structure that can be exploited by sequential generation</li> <li>There is uncertainty associated with the reward, and thus diversity is important</li> <li>The reward function of interest is multi-modal.</li> </ul> <h2 id="learning-in-gflownets">Learning in GFlowNets</h2> <p>Let us look at how we can learn \(\pi\). Each complete trajectory in \(\mathcal{G}\) is assigned a <em>trajectory flow</em>, \(F(\tau)\). This flow represents the unnormalized probability mass associated with the trajectory. We can also define the <em>edge flow</em>, \(F(s \rightarrow s') = \sum_{s\rightarrow s' \in \tau}F(\tau)\), which is the sum of flows of all trajectories containing the edge. A key idea in GFlowNets is using the flows to drive the sequential generation of objects. To this end, using the flows, we can define a <em>forward policy</em> \(P_F(-|s)\), which describes how to choose the next next action (addition of a building block) at a state. This forward policy is defined as \(P_F(s'|s)= \frac{F(s\rightarrow s')}{\sum_{s''\in\text{Child}(s)}F(s\rightarrow s'')}\). </p> <p>We can generate trajectory \(\tau\) by iteratively sampling actions from the forward policy. As the actions at each state are assumed to be independent of the previous states, the likelihood of a trajectory under the forward policy is given by \(P_F(\tau) = \prod_{s\rightarrow s' \in \tau}P_F(s'|s)\). As noted earlier, there can be multiple trajectories resulting in the same object \(x\). The probability of generating an object \(x\) following \(P_F\), i.e. \(\pi(x)\) is given by \(\sum_{\tau=(s_0\rightarrow \dots\rightarrow x)}P_F(\tau)\), by considering all the trajectories resulting in \(x\). The learning problem in GFlowNets is to learn approximate flow functions such that the probability of generating \(x\), \(\pi(x)\) is proportional to its reward. </p> \[\pi(x) = \frac{R(x)}{Z}\] <p>When this equation is satisfied, \(Z\) denotes the partition function of the unnormalized distribution represented by the reward function, \(Z = \sum_{x\in\mathcal{X}}R(x)\). Approaches to tackle this learning problem generally involve learning an approximate flow function, and or approximate forward policies. These are approximated with neural networks operating on states \(s \in \mathcal{S}\).</p> <p><strong>Flow Matching</strong></p> <p>A flow \(F\) is <em>consistent</em> if the outgoing flow at each non-terminal state \(s\) matches the incoming flow.</p> \[\sum_{s''\in \text{Parent}(s)}F(s''\rightarrow s) = \sum_{s'\in \text{Child}(s)}F(s\rightarrow s')\] <p>This is similar to the notion of <em>feasible flows</em> in graph theory, and bears resemblance to the conservation laws in physics. Using this we can discuss a key result in GFlowNet, initially presented in <d-cite key="bengio2021flow"></d-cite></p> <details><summary>💡 <strong>Flow Matching Criterion</strong></summary> <p>Given a consistent flow \(F\), with the incoming flow at terminal states set equal to the reward, \(\sum_{s'\rightarrow x\in \mathcal{E}}F(s'\rightarrow x) = R(x)\), the marginal likelihood of sampling an object \(x\) is proportional to the reward \(\pi(x) = \frac{R(x)}{Z}\).</p> </details> <p>In other words if the flow is conserved at all states, then sampling trajectories following the flow results in reward proportional sampling. This elegant result leads to a relatively straightforward approach for tackling the GFlowNet learning problem - learn parameters \(\theta\) for the edge flow function \(F(s\rightarrow s';\theta)\), which is typically a neural network - resulting in the following flow matching objective</p> \[\mathcal{L}_{FM}(\tau;\theta) = \sum_{s\ne s_0\in\tau}\left(\sum_{s''\in \text{Parent}(s)}F(s''\rightarrow s;\theta) - R(s) - \sum_{s'\in \text{Child}(s)}F(s\rightarrow s';\theta)\right)^2\] <p>where \(R(s)\) is \(0\) for all terminal states and equal to the reward \(R(x)\) for the terminal states. We can already notice a key property of the learning objective - it is <em>off-policy.</em> What that means is that we can use any trajectory, not just ones sampled from the current policy, for training. This allows us to use exploratory policies to collect trajectories for training and even use offline data!</p> <p><strong>Subtrajectory Balance</strong></p> <p>Like temporal-difference learning objectives in RL, the flow matching objective, however, can suffer from slow credit assignment in long trajectories. This is addressed by the family of trajectory balance objectives. In particular, subtrajectory balance, introduced in <d-cite key="madan2023learning,malkin2022trajectory"></d-cite>, is a learning objective which captures several other GFlowNet learning objectives. Before we look at the subtrajectory balance objective, we define the <em>backward policy \(P_B\)</em> which is a necessary ingredient. Like the forward policy \(P_F\) defines a distribution over the children of a state, \(P_B\) defines a distribution over the parents of a state. With \(P_B\) we can generate a trajectory backwards starting at a terminal states \(x\). Let us now look at the subtrajectory balance objective</p> \[\mathcal{L}_{SubTB}(\tau=(s_m\rightarrow \dots\rightarrow s_n);\theta) = \left(\log\frac{F(s_m;\theta\prod_{i=m}^{n-1}P_F(s_{i+1}|s_{i};\theta)}{F(s_n;\theta\prod_{i=m}^{n-1}P_B(s_{i}|s_{i+1};\theta)} \right)^2\] <p>where \(\theta\) are the learnable parameters for \(P_F, P_B, F\). An interesting property of the objective is that it can operates on any subtrajectory. During training we consider subtrajectories from trajectroies sampled using the current policy. As opposed to the flow matching loss where credit is propagated over multiple trajectories, here the credit is assigned to all states in the subtrajectory resulting in lower variance in the gradients and faster convergence. I refer the curious readers to <d-cite key="madan2023learning">&lt;/d=cite&gt; for a detailed look at learning objectives for GFlowNets.</d-cite></p> <p>We can define a general training algorithm for GFlowNets as follows</p> <ul> <li>Initialize parameters \(\theta\) for the flow function \(F(;\theta)\) and/or policies \(P_F(-|-; \theta), P_B(-|-;\theta), Z_\theta\)</li> <li>Sample trajectories following the forward policy \(\tau\sim P_F\)</li> <li>Compute loss for the trajectory \(\mathcal{L}(\tau;\theta)\)</li> <li>Update parameters \(\theta\) with gradients from the loss.</li> </ul> <p>There is a growing literature around the mathematical foundations of GFlowNets which is too extensive to cover in a single post. I recommend <d-cite key="bengio2021gflownet"></d-cite> for a deeper mathematical study of GFlowNets.</p> <h1 id="promise-of-gflownets-for-scientific-discovery">Promise of GFlowNets for Scientific Discovery</h1> <h3 id="generating-diverse-and-useful-experiments">Generating Diverse and Useful Experiments</h3> <p>Our initial work on GFlowNets <d-cite key="bengio2021flow"></d-cite> was motivated by the problem of diverse molecule generation. In particular, the goal was to generate molecules that bind to a particular target, and potentially inhibit the activity of the target. We considered soluble epoxide hydrolase (sEH) in it’s 4JNC configuration, which plays a role in certain respiratory and heart diseases, as the target. To simulate the action of the designed ligand on the sEH target, we relied on docking simulations from Autodock Vina. However, each simulation takes about 5 minutes to run, thus learning a policy with the docking score as reward directly would be prohibitively expensive. Instead, we train a graph neural network, which is much faster to query for a reward, using a dataset of docking scores for 300,000 molecules which is used a <em>proxy</em> for the true reward.</p> <p>We looked at fragment based generation of molecules - the policy picks molecular subgraphs, rather than individual atoms, to put together for constructing the molecule graph. These fragments are generated from the Zinc database. This problem possesses all the 3 properties for GFlowNets to be effective - compositionality - the molecules are composed of the subgraphs with unique chemical properties, uncertainty in the reward - the reward is approximated by a neural network which will have some uncertainty associated to it, and the reward is multimodal - we can expect several molecules to bind well to a given target. Consequently, GFlowNets are able to substantially outperform other methods, generating <strong><em>diverse</em></strong> molecules with <strong>low binding energy</strong>. In particular GFlowNets discover significantly more modes of the reward function relative to other methods. Further we also consider an active learning setup where we start with a dataset of 2000 molecules and use the docking simulation as the oracle. We find that generating batches to be queried with GFlowNets results in significant performance improvements, resulting in discovery of much lower energy molecules than those in the initial dataset.</p> <p>Further exploring the active learning setting, we considered additional improvements to GFlowNets to improve performance in the active learning setting <d-cite key="jain2022biological"></d-cite>. The improvements were two-fold: (a) incorporating offline data to improve sample efficiency, and (b) incorporating the epistemic uncertainty from the approximate reward model to guide search to novel areas of the state space. With these improvements we demonstrated that GFlowNets can generate novel and diverse biological sequences, in particular antimicrobial peptides, which have significant potential for therapeutic use.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/gfn_sd_fig3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/gfn_sd_fig3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/gfn_sd_fig3-1400.webp"></source> <img src="../../../assets/img/blog/gfn_sd_fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Schematic of GFlowNet-AL, incorporating GFlowNets of generation of diverse candidates in active learning.</figcaption> </figure> <p>In many practical settings, there can be multiple properties of interest, where we want to generate experiments which capture them simultaneously. For example, in the context of drug discovery we would like to generate molecules that inhibit the target but which are also synthesizable and not toxic to humans. Multi-Objective GFlowNets <d-cite key="jain2023multi"></d-cite> are extensions of GFlowNets to tackle problems with multiple objectives being optimized simultaneously. Multi-Objective GFlowNets decompose the multi-objective optimization problem into a family of sub-problems which can modelled simultaneously. Through experiments on a wide variety of tasks ranging from small molecule generation to protein design, we show that Multi-Objective GFlowNets generate <em>diverse and Pareto-optimal</em> candidates in multi-objective optimization. In particular, we show that GFlowNets can generate molecules that bind to a target while being synthesizable and having drug-like properties. These initial empirical results have demonstrated the ability of GFlowNets to have significant impact in realistic experimental design scenarios.</p> <h3 id="learning-predictive-models">Learning Predictive Models</h3> <p>While still in early stages, recent work has also demonstrated the ability of GFlowNets to model predictive posteriors from data.</p> <p>DAG-GFlowNets<d-cite key="deleu2022bayesian"></d-cite> demonstrated how GFlowNets can be used to model Bayesian posteriors. They study the problem of learning the Bayesian posterior over graphical models that fit well the data. Through experiments on standard causal discovery tasks, they establish the ability of GFlowNets to accurately model the posterior over the structure of the underlying causal graph of the data. This posterior over the causal structure can in-turn enable targeted uncertainty estimation. For example, given gene knockout data, the posterior over causal graphs that fit the data can reveal the uncertainty in our model about specific causal links.</p> <p>Another popular paradigm for practical scenarios is that of approximate Bayesian inference. Methods such as Monte-Carlo dropout and ensembles have become default choices to represent the uncertainty over neural network parameters. Pushing this direction further, our recent work <d-cite key="liu2023gflowout"></d-cite> leverages GFlowNets to generate dropout masks, to obtain a more faithful and reliable predictive posterior, by generating <strong>diverse</strong> dropout masks.</p> <h2 id="looking-forward">Looking Forward</h2> <p>With this glowing discussion and <em>diverse</em> range of applications, you might be tempted to ask “Are GFlowNets all you need?” But as with most questions of this form, the answer is certainly “No”.</p> <p>What GFlowNets <em>do</em> offer is a flexible probabilistic modeling framework that paves the way for developing approaches to accelerate scientific discovery. There are still several open challenges that need to be addressed in the context of GFlowNets: Multi-fidelity GFlowNets, Intervention Policies, Exploration, and many more! We expand on these ideas, positioning GFlowNets as a key tool to accelerate scientific discovery in <d-cite key="jain2023gflownets"></d-cite>.</p> <p>To get started with GFlowNets, checkout <a href="https://colab.research.google.com/drive/1fUMwgu2OhYpQagpzU5mhe9_Esib3Q2VR" rel="external nofollow noopener" target="_blank">this tutorial Colab</a> by Emmanuel Bengio, and this list of <a href="https://github.com/zdhNarsil/Awesome-GFlowNets" rel="external nofollow noopener" target="_blank">Awesome-GFlowNets</a> by Dinghuai Zhang!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Yoshua Bengio and Emmanuel Bengio provided valuable comments and feedback on this post. My collaborators and mentors helped shape the ideas discussed here through numerous discussions.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-03-07-gflownet-scientific-discovery.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Moksh Jain. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: October 14, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>